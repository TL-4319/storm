STORM
======
A python software for Storm Tracking On-orbit using RFS with Complementary MOS (STORM) developed by the Laboratory for Autonomy, GNC, and Estimation Research (LAGER) at the University of Alabama (UA)

## Dev environment
To facilitate development of STORM which requires concurrent development in CARBS, ```git submodules``` is used. CARBS submodule tracks the ```ggiw-dev``` [branch](https://github.com/drjdlarson/carbs/tree/ggiw-dev).

Clone the repository locally on your computer, using

```
git clone --recurse-submodules https://github.com/TL-4319/storm.git
```

For windows it is recommended to clone it within your linux subsystem directory (e.g. a sub-directory of your linux home folder) to improve performance within the container (the linux directories on Windows can be accessed through the file browser by typing ```\\wsl$``` in the address bar and clicking on your distro).

It is highly recommended to use VS Code with the dev containers extension. 

## Workflow
After a dev container is spun up, the shell is logged in as non-root user ```vscode``` so that any file created is not root-acess limited in the host machine. NOTE: vscode still has root proviledge in the containter.

The terminal by default will not have a shell environment i.e. shows up as just ```$```. Enter ```bash``` to begin a bash environment for nice features.

### Development in CARBS
After implementing new feature in CARBS submodule and ready for test withint the container, build CARBS locally. NOTE: Don't forget "./" prefixing carbs to install the correct version.

```
pip3 install -e ./carbs
```

CARBS submodule is tracked via its own git and needs to be commit and push from within ```storm/carbs```. Submodule git commit then follows accordingly.

### Developing STORM application
If a code requires import from CARBS, make sure to build the latest local CARBS package.

## Preprocessing data
LIS data from ISS can be acquired from [here](https://search.earthdata.nasa.gov/search?q=lightning&fi=LIS). Make sure to get both science and background data in netCDF format. Files are to be saved in ```storm/dataset``` directory.

#### Generate background video
Videos from background LIS images are generated by running ```make_bg_video.py```. Modify ```granule``` variable to reflect which orbit you want to pre-process. You can also change the size of the output video for visual clarity. ```dt``` change the time step of the video. Image frames are constant for interpolation in the time vector as frames are capture at different timescale compare to the ```dt```. The video is overlayed with TAI93 timestamps. Video is output as ```storm/dataset/<granule>/background.avi```.

#### Generate measurement data
Raw LIS data can be prepared into measurement tables using ```pre_process_event.py``` script by first modifying the ```granule``` variable to select the desired dataset. ```dt``` allows control over the timescale of the dataset. As LIS has framerate upto 500Hz, measurement are group into time bin with widths equal to ```dt```. The script output a video of the events along with TAI93 timestamps in ```storm/dataset/<granule>/events.avi```. The script also generates a pickle file in ```storm/dataset/<granule>/events.pik``` which contains dictionary ```meas_data``` which contains the following keys for Z number of measurements step:
- ```TAI93_time_s``` [Z x 1] numpy array
- ```elapsed_time_s``` [Z x 1] numpy array. Essentially ```TAI93_time_s - TAI93_time_s[0]```
- ```meas_tab``` list of size [Z] where ```meas_tab[k] = []``` if there are no measurement at k. If k has measurement then ```meas_tab[k] = meas_list``` where ```meas_list``` is a list with n [2 x 1] numpy array(s) where n is the number of measurement at that time step. The first and second row contains the X and Y pixel location, respectively. 

## Toy example
A quick simulation is included in ```et-test.py``` which contain 2D constant velocity model with a bias in Y-velocity to emulate LEO sensor motion over earth. The observation space is limit to a square window of 128 x 128. The extended targets is a ```list``` that contains three terms:
-   ```rate``` : Poisson rate of measurement (lightning events). A function of the target shape and is estimated using [this](./ref/lightning_mapper/Journal%20of%20Geophysical%20Research%20%20Atmospheres%20-%202008%20-%20Deierling%20-%20Total%20lightning%20activity%20as%20an%20indicator%20of%20updraft.pdf).
- ```state``` [4 x 1] numpy array : Kinematic states of the target [x_pos, x_vel, y_pos, y_vel] 
- ```shape``` [6 x 1] numpy array : Shape state of the target [theta, theta_dot, semi_major, semi_major_dot, semi_minor, semi_minor_dot]

The measurement set ```Z``` is generated with the following scheme:
- Check if target is in FOV. FOV is true when any extent of the target is within the FOV
- Randomly set if target generate meas or not in this timestep. Probability defined by ```lightning_prob```
- If set to generate measurement, sample for number of measurement ```|Z|``` from ```Pois(|Z|,rate)```
- Sample ```|Z|``` number measurements ```Z = {m1, m2, ...} ``` with spatial distribution follow ing ```Norm(m, pos, 0.25 * shape_mat)```. Note, the shape is tightened to shrink the measurement span similar to method used [here](/ref/GGIW/Tracking_of_Extended_Objects_and_Group_Targets_Using_Random_Matrices.pdf)
- Cull any measurement not in FOV

The returned measurement set is returned as ```2 x |Z| numpy array``` where ```meas_in[0,:]``` are the detections X positions and ```meas_in[1,:]``` are the detection Y positions.

## Implementation Details
### Clustering
The measurement clustering step is implemented as a wrapper in ```carbs_clustering.py``` to provide a common interface separately from the underlying clustering algorithm used.

As the end-user, the ```MeasurementClustering``` class is used. This class includes

-   ```__init___```: which can take in a ```dict``` that include the desired method along with its corresponding settings. 
-   ```cluster```: which takes in a list of [D x 1] numpy arrays. It returns a list of clusters where each cluster is a list of partition [D x 1] numpy arrays.

### Example usage
Currently, only DBSCAN clustering is implemented. To construct a clusterer object, the methods and the appropriate settings must be set. This can be done by initializing an appropriate ```Parameters``` object defined in ```carbs_clustering.py```. For example, for DBSCAN, create a parameter object dict by

```
cluster_params = carbs_clustering.DBSCANParameters()
```

Here ```DBSCANParameters``` can takes in optional parameters to override defaults settings. We are using scikit's DBSCAN implementation so a list of settings can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html). In addition, DBSCAN return all noisy points as the same group with label "-1". This behavior is not applicable to GGIW-RFS filter so a flag ```ignore_noise``` was added to either remove all those points or retain them but have them in an idividual group.

Once a ```Parameters``` object is created, it can be passed while constructing ```MeasurementClustering``` to set the clustering code with the appropriate method and setting. Once constructed, a measurement set can be clustered by

```
clusterer = carbs_clustering.MeasurementClustering(clustering_params)

clustered_list = clusterer(original_list)
```
Example of full usage can be found in ```test_clustering.py```





